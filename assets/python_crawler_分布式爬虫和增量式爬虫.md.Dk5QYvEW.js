import{_ as s,c as a,o as l,a2 as e}from"./chunks/framework.CQECOx-R.js";const k=JSON.parse('{"title":"分布式爬虫和增量式爬虫","description":"","frontmatter":{},"headers":[],"relativePath":"python/crawler/分布式爬虫和增量式爬虫.md","filePath":"python/crawler/分布式爬虫和增量式爬虫.md","lastUpdated":1746516944000}'),p={name:"python/crawler/分布式爬虫和增量式爬虫.md"};function t(r,i,n,h,d,o){return l(),a("div",null,i[0]||(i[0]=[e(`<h1 id="分布式爬虫和增量式爬虫" tabindex="-1">分布式爬虫和增量式爬虫 <a class="header-anchor" href="#分布式爬虫和增量式爬虫" aria-label="Permalink to &quot;分布式爬虫和增量式爬虫&quot;">​</a></h1><h2 id="分布式爬虫" tabindex="-1">分布式爬虫 <a class="header-anchor" href="#分布式爬虫" aria-label="Permalink to &quot;分布式爬虫&quot;">​</a></h2><h3 id="概念" tabindex="-1">概念 <a class="header-anchor" href="#概念" aria-label="Permalink to &quot;概念&quot;">​</a></h3><p>搭建集群，让集群对一组资源进行联合爬取</p><h3 id="作用" tabindex="-1">作用 <a class="header-anchor" href="#作用" aria-label="Permalink to &quot;作用&quot;">​</a></h3><p>提升爬取数据效率</p><h3 id="实现" tabindex="-1">实现 <a class="header-anchor" href="#实现" aria-label="Permalink to &quot;实现&quot;">​</a></h3><ul><li><p>安装scrapy-redis组件</p></li><li><p>原生scrapy无法实现分布式爬虫</p><ul><li>调度器不可被集群共享</li><li>管道不可被集群共享</li></ul></li><li><p>scrapy-redis组件作用</p><ul><li>给原生scrapy提供被共享的调度器和管道</li></ul></li><li><p>实现流程</p><ul><li><p>创建工程</p></li><li><p>创建一个基于CrawlSpider的爬虫</p></li><li><p>修改爬虫文件</p><ul><li><p>爬虫文件添加<code>from scrapy_redis.spiders import RedisCrawlSpider</code></p></li><li><p>注释掉start_urls和allowed_domains</p></li><li><p>新增属性<code>redis_key = &#39;story&#39;</code>，代表被共享的调度器队列的名称</p></li><li><p>编写数据解析操作</p></li><li><p>将当前爬虫类的父类修改成<code>RedisCrawlSpider</code></p></li></ul></li><li><p>settings配置新增</p><ul><li><p>指定使用可以共享的管道</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code><span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">ITEM_PIPELINES</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> {</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    &#39;scrapy_redis.pipelines.RedisPipeline&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> : </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">400</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">}</span></span></code></pre></div></li><li><p>指定可以共享的调度器</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 增加一个去重容器的配置,使用redis的set来存储请求数据,实现请求去重持久化</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">DUPEFLTER_CLASS</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;scrapy_redis.dupefilter.RFPDupeFilter&quot;</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 使用scrapy-redis组件自己的调度器</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">SCHEDULER</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;scrapy_redis.scheduler.Scheduler&quot;</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 配置调度器是否要持久化-爬虫结束要不要清空redis请求队列和去重的set</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">SCHEDULER_PERSIST</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> True</span></span></code></pre></div></li></ul></li><li><p>配置redis的配置文件</p><ul><li><code>bind 127.0.0.1</code>注释掉</li><li>关闭保护模式:<code>protected-mode</code> 改为no</li></ul></li><li><p>启动redis</p></li><li><p>启动工程，进入到爬虫文件的目录后<code>scrapy runspider xxx</code></p></li><li><p>向调度器队列中放入起始url</p><ul><li><code>lpush redis_key url</code></li></ul></li></ul></li></ul><h2 id="增量式爬虫" tabindex="-1">增量式爬虫 <a class="header-anchor" href="#增量式爬虫" aria-label="Permalink to &quot;增量式爬虫&quot;">​</a></h2><h3 id="概念-1" tabindex="-1">概念 <a class="header-anchor" href="#概念-1" aria-label="Permalink to &quot;概念&quot;">​</a></h3><p>检测网站数据更新情况，只会爬取网站最新更新的数据</p><h3 id="实现-1" tabindex="-1">实现 <a class="header-anchor" href="#实现-1" aria-label="Permalink to &quot;实现&quot;">​</a></h3><ul><li>指定起始url</li><li>基于CrawlSpider获取其他页码链接</li><li>基于Rule对其他页码进行请求</li><li>从每一个页码对应源码中解析出详情页的url</li><li>==检测详情页的url是否被请求过（redis/mysql)==</li><li>对详情页发起请求</li><li>持久化存储</li></ul>`,13)]))}const u=s(p,[["render",t]]);export{k as __pageData,u as default};
